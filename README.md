# VisionAssist - AI-Powered Visual Accessibility

<div align="center">

ğŸ”® **Empowering independence through AI vision technology**

[![Next.js](https://img.shields.io/badge/Next.js-16.1-black?style=for-the-badge&logo=next.js)](https://nextjs.org/)
[![Google Gemini](https://img.shields.io/badge/Gemini-AI-blue?style=for-the-badge&logo=google)](https://ai.google.dev/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue?style=for-the-badge&logo=typescript)](https://www.typescriptlang.org/)
[![Tailwind CSS](https://img.shields.io/badge/Tailwind-4.0-38B2AC?style=for-the-badge&logo=tailwind-css)](https://tailwindcss.com/)

</div>

## ğŸŒŸ Overview

**VisionAssist** is an innovative AI-powered accessibility application designed to help visually impaired individuals understand and navigate their environment. Using advanced computer vision powered by Google's Gemini AI, VisionAssist provides real-time scene descriptions, text reading, object identification, and navigation assistance.

## âœ¨ Features

### ğŸ¯ Core Capabilities

| Mode | Description |
|------|-------------|
| ğŸ‘ï¸ **Scene Description** | Get detailed descriptions of your surroundings including objects, people, and spatial layout |
| ğŸ“– **Text Reading** | Extract and read text from documents, signs, labels, and more |
| ğŸ“¦ **Object Identification** | Identify objects, products, brands, and get usage information |
| ğŸ§­ **Navigation Assistance** | Receive guidance about paths, obstacles, and safe navigation routes |
| ğŸ¨ **Color Detection** | Identify colors in scenes for clothing, food, and other color-dependent tasks |

### â™¿ Accessibility Features

- **ğŸ”Š Text-to-Speech**: All results are automatically read aloud
- **ğŸ¤ Voice Commands**: Control the app hands-free with voice commands
- **âŒ¨ï¸ Keyboard Navigation**: Full keyboard support with shortcuts
- **ğŸŒ“ High Contrast Mode**: Enhanced visibility for low-vision users
- **ğŸ“ Adjustable Text Size**: Customize text size from 14px to 28px
- **ğŸ–¥ï¸ Screen Reader Compatible**: Proper ARIA labels and semantic HTML

## ğŸš€ Getting Started

### Prerequisites

- Node.js 18+ 
- npm or yarn
- Google Gemini API key ([Get one here](https://makersuite.google.com/app/apikey))

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/visionassist.git
   cd visionassist
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Set up environment variables**
   ```bash
   cp .env.local.example .env.local
   ```
   
   Edit `.env.local` and add your Gemini API key:
   ```
   GEMINI_API_KEY=your_api_key_here
   ```

4. **Run the development server**
   ```bash
   npm run dev
   ```

5. **Open your browser**
   Navigate to [http://localhost:3000](http://localhost:3000)

## âŒ¨ï¸ Keyboard Shortcuts

| Key | Action |
|-----|--------|
| `1-5` | Switch between analysis modes |
| `Space` | Capture image (when camera is active) |
| `C` | Open camera |
| `S` | Stop speech |
| `R` | Repeat last description |
| `V` | Activate voice command |
| `Esc` | Close camera/settings |

## ğŸ¤ Voice Commands

- **"Scene"** or **"Describe"** - Switch to scene description mode
- **"Read"** or **"Text"** - Switch to text reading mode
- **"Identify"** or **"Object"** - Switch to object identification mode
- **"Navigate"** or **"Path"** - Switch to navigation mode
- **"Color"** - Switch to color detection mode
- **"Capture"** or **"Take photo"** - Capture current camera view
- **"Stop"** or **"Quiet"** - Stop current speech
- **"Repeat"** or **"Again"** - Repeat last description

## ğŸ› ï¸ Technology Stack

- **Frontend**: Next.js 16.1, React 19, TypeScript
- **Styling**: Tailwind CSS 4.0
- **AI**: Google Gemini 1.5 Flash
- **Icons**: Lucide React
- **APIs**: Web Speech API (TTS & STT), MediaDevices API

## ğŸ“ Project Structure

```
visionassist/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ analyze/
â”‚   â”‚   â”‚       â””â”€â”€ route.ts      # Gemini AI integration
â”‚   â”‚   â”œâ”€â”€ globals.css           # Global styles
â”‚   â”‚   â”œâ”€â”€ layout.tsx            # Root layout
â”‚   â”‚   â””â”€â”€ page.tsx              # Main page
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ VisionAssist.tsx      # Main application component
â”‚   â””â”€â”€ types/
â”‚       â””â”€â”€ speech.d.ts           # Speech API types
â”œâ”€â”€ public/                        # Static assets
â”œâ”€â”€ .env.local.example             # Environment template
â””â”€â”€ package.json
```

## ğŸ¯ Use Cases

1. **Daily Navigation**: Help users navigate indoor and outdoor environments safely
2. **Reading Mail**: Read letters, bills, and documents
3. **Shopping**: Identify products, read labels, and check prices
4. **Clothing Selection**: Identify colors and patterns for outfit coordination
5. **Cooking**: Read recipes and identify ingredients
6. **Public Spaces**: Navigate airports, malls, and transit stations

## ğŸ”’ Privacy & Security

- **No image storage**: Images are processed in real-time and not stored
- **Local processing**: Voice commands are processed locally when possible
- **Secure API**: All API calls are made server-side
- **No tracking**: No user tracking or analytics

## ğŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines for more details.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Google Gemini AI for powering the vision capabilities
- The accessibility community for guidance and feedback
- Open source contributors

---

<div align="center">

**Built with â¤ï¸ for accessibility**

[Report Bug](https://github.com/yourusername/visionassist/issues) Â· [Request Feature](https://github.com/yourusername/visionassist/issues)

</div>
